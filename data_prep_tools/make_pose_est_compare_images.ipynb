{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T04:16:54.550428Z",
     "start_time": "2020-09-27T04:16:54.535250Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "notebook_fixed_dir = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T04:16:55.044622Z",
     "start_time": "2020-09-27T04:16:55.032081Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/svcl-oowl/brandon/research/sil_consistent_at_inference\n"
     ]
    }
   ],
   "source": [
    "# this cell can only be called once\n",
    "import os\n",
    "if not notebook_fixed_dir:\n",
    "    os.chdir('..')\n",
    "    notebook_fixed_dir = True\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T04:23:19.356901Z",
     "start_time": "2020-09-27T04:23:19.326737Z"
    }
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pytorch3d.renderer import look_at_view_transform\n",
    "import matplotlib.pyplot as plt\n",
    "import trimesh\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from utils import utils\n",
    "from utils.brute_force_pose_est import brute_force_estimate_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T04:25:21.938991Z",
     "start_time": "2020-09-27T04:25:21.912703Z"
    }
   },
   "outputs": [],
   "source": [
    "# given an input directory of meshes, and an input directory of corresponding images,\n",
    "# predicts the pose, renders the meshes at the predicted pose, and saves it together with the input image \n",
    "# side-by-side. The resulting output folder of images is useful for later visual evaluation of pose accuracy.\n",
    "\n",
    "INPUT_IMG_DIR = \"data/img_pix3d_chair/chair\"\n",
    "INPUT_MESH_DIR =  \"data/onet_chair_pix3d_no_DA_simplified\"\n",
    "COMPARES_OUTPUT_DIR = \"data_prep_tools/pose_est_compare/pix3d_chair_occnet\"\n",
    "\n",
    "# an optional processed folder, with precomputed poses. \n",
    "# If set as an empty string, poses will be recomputed from scratch\n",
    "PROCESSED_MESH_DIR = \"data/onet_chair_pix3d_no_DA_simplified_processed\"\n",
    "\n",
    "num_azims = 20\n",
    "num_elevs = 20\n",
    "num_dists = 40\n",
    "\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T04:25:23.191085Z",
     "start_time": "2020-09-27T04:25:23.156222Z"
    }
   },
   "outputs": [],
   "source": [
    "if PROCESSED_MESH_DIR != \"\":\n",
    "    cached_pred_poses = {}\n",
    "    pred_pose_paths = list(Path(PROCESSED_MESH_DIR).rglob('pred_poses.p'))\n",
    "    for pred_pose_path in pred_pose_paths:\n",
    "        curr_cache = pickle.load(open(pred_pose_path, \"rb\"))\n",
    "        cached_pred_poses = {**cached_pred_poses, **curr_cache}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T04:25:24.244306Z",
     "start_time": "2020-09-27T04:25:24.213961Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_concat_h(im1, im2):\n",
    "    dst = Image.new('RGB', (im1.width + im2.width, im1.height), color=(0, 255, 255))\n",
    "    dst.paste(im1, (0, 0))\n",
    "    dst.paste(im2, (im1.width, 0))\n",
    "    return dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T04:54:15.098909Z",
     "start_time": "2020-09-27T04:26:45.452849Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25d52b590c0449b1bb67cb8ae8ce4122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3838.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(COMPARES_OUTPUT_DIR):\n",
    "    os.makedirs(COMPARES_OUTPUT_DIR)\n",
    "instance_names = [path.split('/')[-1].replace(\".obj\", \"\") for path in glob.glob(os.path.join(INPUT_MESH_DIR, \"*.obj\"))]\n",
    "\n",
    "for instance_name in tqdm(instance_names):\n",
    "    img_path = os.path.join(INPUT_IMG_DIR, instance_name + \".png\")\n",
    "    mesh_path = os.path.join(INPUT_MESH_DIR, instance_name + \".obj\")\n",
    "    image = Image.open(img_path)\n",
    "    mask = np.asarray(image)[:,:,3] > 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        mesh = utils.load_untextured_mesh(mesh_path, device)\n",
    "        if PROCESSED_MESH_DIR == \"\":\n",
    "            _, _, _, render, _ = brute_force_estimate_pose(mesh, mask, num_azims, num_elevs, num_dists, device, 8)\n",
    "        else:\n",
    "            cached_dist = cached_pred_poses[instance_name][\"dist\"]\n",
    "            cached_elev = cached_pred_poses[instance_name][\"elev\"]\n",
    "            cached_azim = cached_pred_poses[instance_name][\"azim\"]\n",
    "            R, T = look_at_view_transform(cached_dist, cached_elev, cached_azim) \n",
    "            render = utils.render_mesh(mesh, R, T, device)[0]\n",
    "    \n",
    "    render_image = Image.fromarray((render[..., :3].cpu().numpy() * 255).astype(np.uint8)).resize((224, 224))\n",
    "    input_image_rgb = Image.new(\"RGB\", image.size, (255, 255, 255))\n",
    "    input_image_rgb.paste(image, mask=image.split()[3])\n",
    "    pred_pose_compare_img = get_concat_h(input_image_rgb, render_image)\n",
    "    pred_pose_compare_img.save(os.path.join(COMPARES_OUTPUT_DIR,\"{}.png\".format(instance_name)))\n",
    "    \n",
    "    #plt.imshow(pred_pose_compare_img)\n",
    "    #plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
